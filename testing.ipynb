{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "\n",
    "from data_loader import prepare_ml_pipeline\n",
    "from matrix_factor import BiasedMF, train_mf\n",
    "from debiasing import train_debiasing_model\n",
    "from dropoutnet import train_dropoutnet\n",
    "from evaluator import ndcg_calc_sampled, ndcg_calc_dropout_sampled, ndcg_calc_debiased_sampled, evaluate_split\n",
    "import evaluator as ev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_debiasing_transformation(model, R, cold_items=None):\n",
    "    \"\"\"\n",
    "    Analyze how the debiasing model transforms ratings\n",
    "    \n",
    "    Args:\n",
    "        model: Debiasing model (or None for pre-debiasing analysis)\n",
    "        R: Input rating matrix (items x users)\n",
    "        cold_items: Optional mask/indices of cold items\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Convert inputs to numpy\n",
    "        R_orig = R.cpu().numpy()\n",
    "        \n",
    "        # If we have a debiasing model, get transformed ratings\n",
    "        if model is not None:\n",
    "            model.eval()\n",
    "            R_debiased = model(R, is_training=False).preds\n",
    "            R_deb = R_debiased.cpu().numpy()\n",
    "        else:\n",
    "            R_deb = R_orig\n",
    "        \n",
    "        # Overall distribution\n",
    "        print(\"\\nRating Distribution:\")\n",
    "        print(f\"Original - Mean: {R_orig.mean():.4f}, Std: {R_orig.std():.4f}\")\n",
    "        if model is not None:\n",
    "            print(f\"Debiased - Mean: {R_deb.mean():.4f}, Std: {R_deb.std():.4f}\")\n",
    "        \n",
    "        # Analyze by rating strength\n",
    "        orig_percentiles = np.percentile(R_orig, [25, 50, 75])\n",
    "        print(\"\\nRating Strength Analysis:\")\n",
    "        print(\"Original Quartiles:\", orig_percentiles)\n",
    "        \n",
    "        # How ratings are distributed\n",
    "        low_mask = R_orig <= orig_percentiles[0]\n",
    "        mid_mask = (R_orig > orig_percentiles[0]) & (R_orig <= orig_percentiles[2])\n",
    "        high_mask = R_orig > orig_percentiles[2]\n",
    "        \n",
    "        print(\"\\nRating Distribution by Strength:\")\n",
    "        print(f\"Low ratings: mean={R_orig[low_mask].mean():.4f}\")\n",
    "        print(f\"Mid ratings: mean={R_orig[mid_mask].mean():.4f}\")\n",
    "        print(f\"High ratings: mean={R_orig[high_mask].mean():.4f}\")\n",
    "        \n",
    "        if model is not None:\n",
    "            print(\"\\nTransformation by Rating Strength:\")\n",
    "            print(f\"Low ratings: {R_orig[low_mask].mean():.4f} -> {R_deb[low_mask].mean():.4f}\")\n",
    "            print(f\"Mid ratings: {R_orig[mid_mask].mean():.4f} -> {R_deb[mid_mask].mean():.4f}\")\n",
    "            print(f\"High ratings: {R_orig[high_mask].mean():.4f} -> {R_deb[high_mask].mean():.4f}\")\n",
    "        \n",
    "        # Analyze cold items if provided\n",
    "        if cold_items is not None:\n",
    "            print(\"\\nCold Item Analysis:\")\n",
    "            cold_orig = R_orig[cold_items]\n",
    "            cold_deb = R_deb[cold_items]\n",
    "            print(f\"Original - Mean: {cold_orig.mean():.4f}, Std: {cold_orig.std():.4f}\")\n",
    "            if model is not None:\n",
    "                print(f\"Debiased - Mean: {cold_deb.mean():.4f}, Std: {cold_deb.std():.4f}\")\n",
    "            \n",
    "            # Calculate average ranks for cold items\n",
    "            if model is not None:\n",
    "                cold_rank_changes = []\n",
    "                n_users = R_orig.shape[1]\n",
    "                \n",
    "                # Calculate ranks per user\n",
    "                for user_idx in range(n_users):\n",
    "                    # Get rankings for this user\n",
    "                    orig_user_ranks = np.argsort(-R_orig[:, user_idx])\n",
    "                    deb_user_ranks = np.argsort(-R_deb[:, user_idx])\n",
    "                    \n",
    "                    # Find ranks of cold items\n",
    "                    cold_indices = np.where(cold_items)[0]\n",
    "                    for item in cold_indices:\n",
    "                        orig_rank = np.where(orig_user_ranks == item)[0][0]\n",
    "                        deb_rank = np.where(deb_user_ranks == item)[0][0]\n",
    "                        # Convert to percentile (0-1 scale)\n",
    "                        orig_percentile = orig_rank / len(orig_user_ranks)\n",
    "                        deb_percentile = deb_rank / len(deb_user_ranks)\n",
    "                        cold_rank_changes.append(orig_percentile - deb_percentile)\n",
    "                \n",
    "                avg_rank_change = np.mean(cold_rank_changes)\n",
    "                improved_count = np.sum(np.array(cold_rank_changes) > 0)\n",
    "                \n",
    "                print(f\"\\nCold Item Rank Changes:\")\n",
    "                print(f\"Average rank improvement: {avg_rank_change:.4f}\")\n",
    "                print(f\"Items with improved ranks: {improved_count} / {len(cold_rank_changes)}\")\n",
    "                print(f\"Average percentile before: {1 - np.mean([c[0] for c in cold_rank_changes]):.4f}\")\n",
    "                print(f\"Average percentile after: {1 - np.mean([c[1] for c in cold_rank_changes]):.4f}\")\n",
    "        \n",
    "        # Return statistics\n",
    "        stats = {\n",
    "            'original_stats': {\n",
    "                'mean': R_orig.mean(),\n",
    "                'std': R_orig.std(),\n",
    "                'percentiles': orig_percentiles\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if model is not None:\n",
    "            stats['debiased_stats'] = {\n",
    "                'mean': R_deb.mean(),\n",
    "                'std': R_deb.std()\n",
    "            }\n",
    "            \n",
    "        if cold_items is not None:\n",
    "            stats['cold_stats'] = {\n",
    "                'original_mean': cold_orig.mean(),\n",
    "                'original_std': cold_orig.std()\n",
    "            }\n",
    "            if model is not None:\n",
    "                stats['cold_stats'].update({\n",
    "                    'debiased_mean': cold_deb.mean(),\n",
    "                    'debiased_std': cold_deb.std(),\n",
    "                    'mean_change': cold_deb.mean() - cold_orig.mean(),\n",
    "                    'rank_changes': cold_rank_changes if model is not None else None\n",
    "                })\n",
    "        \n",
    "        return stats\n",
    "\n",
    "def check_debiasing(dropout_model, original_mf, debiasing_model, ml_data, device):\n",
    "    \"\"\"Run debiasing transformation analysis\"\"\"\n",
    "    print(\"\\nAnalyzing Rating Distributions...\")\n",
    "    \n",
    "    # Get cold items\n",
    "    cold_items, _ = ev.get_item_split(ml_data.train_data, ml_data.test_data)\n",
    "    cold_mask = torch.zeros(ml_data.n_items, dtype=torch.bool, device=device)\n",
    "    cold_mask[list(cold_items)] = True\n",
    "    \n",
    "    # Get base predictions\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings and content\n",
    "        u_emb, i_emb = original_mf.get_embeddings()\n",
    "        u_emb = u_emb.to(device)\n",
    "        i_emb = i_emb.to(device)\n",
    "        \n",
    "        u_content = (torch.tensor(ml_data.user_content, dtype=torch.float32).to(device) \n",
    "                    if ml_data.user_content is not None else None)\n",
    "        i_content = (torch.tensor(ml_data.item_content, dtype=torch.float32).to(device)\n",
    "                    if ml_data.item_content is not None else None)\n",
    "        \n",
    "        # Get DropoutNet predictions\n",
    "        u_encoded, i_encoded = dropout_model.encode(\n",
    "            u_emb,\n",
    "            i_emb,\n",
    "            u_content,\n",
    "            i_content\n",
    "        )\n",
    "        R = torch.mm(i_encoded, u_encoded.t())\n",
    "        \n",
    "        # Add bias terms\n",
    "        all_users = torch.arange(ml_data.n_users, device=device)\n",
    "        all_items = torch.arange(ml_data.n_items, device=device)\n",
    "        user_biases = original_mf.user_bias(all_users).squeeze()\n",
    "        item_biases = original_mf.item_bias(all_items).squeeze()\n",
    "        R += user_biases.unsqueeze(0) + item_biases.unsqueeze(1) + original_mf.global_bias\n",
    "    \n",
    "    # Analyze distributions\n",
    "    stats = analyze_debiasing_transformation(debiasing_model, R, cold_mask)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ndcg_by_positives(ndcg_scores, n_positives, avg_positives):\n",
    "\n",
    "    norm_factor = avg_positives / n_positives  # will be < 1 for cold users (more positives), > 1 for warm users (fewer positives)\n",
    "    return [score * norm_factor for score in ndcg_scores]\n",
    "\n",
    "def analyze_positive_distribution(ml_data, cold_users, warm_users):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of positive ratings between train and test sets\n",
    "    for both cold and warm users\n",
    "    \"\"\"\n",
    "    train_positives_warm = (ml_data.train_data['user_idx'].isin(warm_users) & (ml_data.train_data['rating'] >= 4)).sum()\n",
    "    test_positives_warm = (ml_data.test_data['user_idx'].isin(warm_users) & (ml_data.test_data['rating'] >= 4)).sum()\n",
    "    total_positives_warm = train_positives_warm + test_positives_warm\n",
    "\n",
    "    train_positives_cold = (ml_data.train_data['user_idx'].isin(cold_users) & (ml_data.train_data['rating'] >= 4)).sum()\n",
    "    test_positives_cold = (ml_data.test_data['user_idx'].isin(cold_users) & (ml_data.test_data['rating'] >= 4)).sum()\n",
    "    total_positives_cold = train_positives_cold + test_positives_cold\n",
    "\n",
    "    print(\"\\nPositive Ratings Distribution Analysis:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Warm Users:\")\n",
    "    print(f\"  Train positives: {train_positives_warm}\")\n",
    "    print(f\"  Test positives: {test_positives_warm}\")\n",
    "    print(f\"  Total positives: {total_positives_warm}\")\n",
    "    print(f\"  Ratio in test: {test_positives_warm/total_positives_warm:.2%}\")\n",
    "    print(f\"  Average per user: {total_positives_warm/len(warm_users):.2f}\")\n",
    "\n",
    "    print(f\"\\nCold Users:\")\n",
    "    print(f\"  Train positives: {train_positives_cold}\")\n",
    "    print(f\"  Test positives: {test_positives_cold}\")\n",
    "    print(f\"  Total positives: {total_positives_cold}\")\n",
    "    print(f\"  Ratio in test: {test_positives_cold/total_positives_cold:.2%}\")\n",
    "    print(f\"  Average per user: {total_positives_cold/len(cold_users):.2f}\")\n",
    "\n",
    "    return {\n",
    "        'warm': {\n",
    "            'train': train_positives_warm,\n",
    "            'test': test_positives_warm,\n",
    "            'total': total_positives_warm,\n",
    "            'test_ratio': test_positives_warm/total_positives_warm\n",
    "        },\n",
    "        'cold': {\n",
    "            'train': train_positives_cold,\n",
    "            'test': test_positives_cold,\n",
    "            'total': total_positives_cold,\n",
    "            'test_ratio': test_positives_cold/total_positives_cold\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def run_evaluation_pipeline(device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"Complete evaluation pipeline with DropoutNet and debiasing\"\"\"\n",
    "    \n",
    "    # Define evaluation parameters\n",
    "    k_values = [15,30]\n",
    "    \n",
    "\n",
    "    dropoutnet_params = {\n",
    "        'model_select': [800, 400],\n",
    "        'rank_out': 200,\n",
    "        'dropout_rate': 0.5,\n",
    "        'batch_size': 1000,\n",
    "        'n_scores_per_user': 2500,\n",
    "        'data_batch_size': 1000,\n",
    "        'max_data_per_step': 50000,\n",
    "        'num_epochs': 1,\n",
    "        'learning_rate': 0.001\n",
    "    }\n",
    "    \n",
    "    #Params from https://github.com/Zziwei/Fairness-in-Cold-Start-Recommendation/blob/main/Scale/main.py\n",
    "    debiasing_params = {\n",
    "        'model_select': [100],\n",
    "        'alpha': 1000.0,\n",
    "        'batch_size': 50,\n",
    "        'num_epochs': 1,\n",
    "        'reg': 0.000001\n",
    "    }\n",
    "    \n",
    "    # 1. Load and prepare data\n",
    "    print(\"Loading data...\")\n",
    "    ml_data, train_loader, valid_loader, test_loader = prepare_ml_pipeline(cold_start=True)\n",
    "  \n",
    "\n",
    "    # Get cold/warm user split\n",
    "    cold_users, warm_users = ev.get_user_split(ml_data.train_data, ml_data.test_data)\n",
    "    cold_items, warm_items = ev.get_item_split(ml_data.train_data, ml_data.test_data)\n",
    "    \n",
    "    # Analyze positive distribution\n",
    "    #distribution_stats = analyze_positive_distribution(ml_data, cold_users, warm_users)\n",
    "    \n",
    "    # Calculate normalized NDCG\n",
    "    cold_positives = len(ml_data.test_data[ml_data.test_data['user_idx'].isin(cold_users)]['rating'] >= 4)\n",
    "    warm_positives = len(ml_data.test_data[ml_data.test_data['user_idx'].isin(warm_users)]['rating'] >= 4)\n",
    "    avg_positives = (cold_positives + warm_positives) / 2\n",
    "    \n",
    "    \n",
    "    # 2. Train and evaluate base model\n",
    "    print(\"\\nTraining base model...\")\n",
    "    base_model = BiasedMF(ml_data.n_users, ml_data.n_items).to(device)\n",
    "    base_model = train_mf(model = base_model, train_loader= train_loader, val_loader= valid_loader, ml_data= ml_data, num_epochs=1, lr = .01)\n",
    "    \n",
    "    base_ndcgs, base_prec, base_recall = ndcg_calc_sampled(base_model, test_loader, ml_data, k_values=k_values)\n",
    "    final_mdg, mdg_anal = ev.mdg_calc_base(base_model, test_loader, ml_data)\n",
    "    print(f\"Base NDCGS: {base_ndcgs}\")\n",
    "    print(f\"Final MDG: {mdg_anal['all']['mean']}, min10: {mdg_anal['bottom_10']['mean']}, min20: {mdg_anal['bottom_20']['mean']}, top10: {mdg_anal['top_10']['mean']}\")\n",
    "    base_cold_warm = ev.analyze_mdg_with_splits(final_mdg, cold_items, warm_items)\n",
    "    print(\"\\nBase Model MDG Analysis:\")\n",
    "    ev.print_mdg_analysis(base_cold_warm)  \n",
    "    \n",
    "    print(\"\\nEvaluating base model...\")\n",
    "    base_metrics = evaluate_split(\n",
    "        eval_model=base_model,\n",
    "        test_loader=test_loader,\n",
    "        ml_data=ml_data,\n",
    "        cold_users=cold_users,\n",
    "        warm_users=warm_users,\n",
    "        k_values=k_values,\n",
    "        evaluation_func=ndcg_calc_sampled\n",
    "    )\n",
    "    \n",
    "    print(\"\\nBase Model Results:\")\n",
    "    print(f\"Base NDCGs: {base_ndcgs}\")\n",
    "    print(\"\\nBase Model Results:\")\n",
    "    print(f\"Cold Users (n={base_metrics.n_cold_users}):\")\n",
    "    #normalized_cold_ndcgs = normalize_ndcg_by_positives(base_metrics.cold_users.ndcg, cold_positives, avg_positives)\n",
    "    print(f\"  Original NDCG@{k_values}: {base_metrics.cold_users.ndcg}\")\n",
    "    #print(f\"  Normalized NDCG@{k_values}: {normalized_cold_ndcgs}\")\n",
    "    \n",
    "    print(f\"Warm Users (n={base_metrics.n_warm_users}):\")\n",
    "    #normalized_warm_ndcgs = normalize_ndcg_by_positives(base_metrics.warm_users.ndcg, warm_positives, avg_positives)\n",
    "    print(f\"  Original NDCG@{k_values}: {base_metrics.warm_users.ndcg}\")\n",
    "    #print(f\"  Normalized NDCG@{k_values}: {normalized_warm_ndcgs}\")\n",
    "\n",
    "    \n",
    "    #3. Train and evaluate DropoutNet model\n",
    "    print(\"\\nTraining DropoutNet model...\")\n",
    "    dropoutnet = train_dropoutnet(\n",
    "        ml_data=ml_data,\n",
    "        base_model=base_model,\n",
    "        val_loader=valid_loader,\n",
    "        test_loader=test_loader,\n",
    "        **dropoutnet_params,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating DropoutNet model...\")\n",
    "    dropout_ndcgs, drop_mdg, drop_mdg_anal = ndcg_calc_dropout_sampled(base_model, dropoutnet, test_loader, ml_data, k_values = k_values)\n",
    "    drop_mdg, drop_mdg_anal = ev.mdg_calc_dropout(dropoutnet, base_model, test_loader,ml_data)\n",
    "    print(f\"Dropout NDCGs {dropout_ndcgs}\")\n",
    "    print(f\"Final MDG: {drop_mdg_anal['all']['mean']}, min10: {drop_mdg_anal['bottom_10']['mean']}, min20: {drop_mdg_anal['bottom_20']['mean']}, top10: {drop_mdg_anal['top_10']['mean']}\")\n",
    "     # Analyze cold vs warm for dropout model\n",
    "    dropout_cold_warm = ev.analyze_mdg_with_splits(drop_mdg, cold_items, warm_items)\n",
    "\n",
    "    print(\"\\nDebiased Model MDG Analysis:\")\n",
    "    ev.print_mdg_analysis(dropout_cold_warm)\n",
    "    \n",
    "    print(\"\\nEvaluating DropoutNet model...\")\n",
    "    dropout_metrics = evaluate_split(\n",
    "        eval_model=dropoutnet,  # Base model needed for embeddings\n",
    "        test_loader=test_loader,\n",
    "        ml_data=ml_data,\n",
    "        cold_users=cold_users,\n",
    "        warm_users=warm_users,\n",
    "        k_values=k_values,\n",
    "        evaluation_func=ndcg_calc_dropout_sampled,\n",
    "        base_model=base_model  # Changed parameter name to avoid conflict\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDropoutNet Results:\")\n",
    "    print(f\"Dropout NDCGs: {dropout_ndcgs}\")\n",
    "    print(f\"Cold Users (n={dropout_metrics.n_cold_users}):\")\n",
    "    #normalized_cold_ndcgs = normalize_ndcg_by_positives(dropout_metrics.cold_users.ndcg, cold_positives, avg_positives)\n",
    "    print(f\"  Original NDCG@{k_values}: {dropout_metrics.cold_users.ndcg}\")\n",
    "    #print(f\"  Normalized NDCG@{k_values}: {normalized_cold_ndcgs}\")\n",
    "    \n",
    "    print(f\"Warm Users (n={dropout_metrics.n_warm_users}):\")\n",
    "    #normalized_warm_ndcgs = normalize_ndcg_by_positives(dropout_metrics.warm_users.ndcg, warm_positives, avg_positives)\n",
    "    print(f\"  Original NDCG@{k_values}: {dropout_metrics.warm_users.ndcg}\")\n",
    "    #print(f\"  Normalized NDCG@{k_values}: {normalized_warm_ndcgs}\")\n",
    "    \n",
    "    # 4. Train and evaluate debiasing model\n",
    "    print(\"\\nTraining debiasing model...\")\n",
    "    debiasing_model = train_debiasing_model(\n",
    "        base_model=dropoutnet,\n",
    "        original_mf=base_model,\n",
    "        ml_data=ml_data,\n",
    "        **debiasing_params,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluating debiased model...\")\n",
    "    debiased_ndcgs, debiased_prec, debiased_rec = ndcg_calc_debiased_sampled(dropoutnet, base_model, debiasing_model,test_loader,ml_data,k_values=k_values)\n",
    "    debiased_mdg, debiased_mdg_anal = ev.mdg_calc_debiased(dropoutnet, base_model, debiasing_model, test_loader, ml_data)\n",
    "    print(f\"Debiased NDCGs {debiased_ndcgs}\")\n",
    "    print(f\"Final MDG: {debiased_mdg_anal['all']['mean']}, min10: {debiased_mdg_anal['bottom_10']['mean']}, min20: {debiased_mdg_anal['bottom_20']['mean']}, top10: {debiased_mdg_anal['top_10']['mean']}\")\n",
    "    # Analyze cold vs warm for debiased model\n",
    "    debiased_cold_warm = ev.analyze_mdg_with_splits(\n",
    "        debiased_mdg,\n",
    "        cold_items, warm_items\n",
    "    )\n",
    "\n",
    "    print(\"\\nDebiased Model MDG Analysis:\")\n",
    "    ev.print_mdg_analysis(debiased_cold_warm)\n",
    "\n",
    "    print(\"\\nEvaluating debiased model...\")\n",
    "    debiased_metrics = evaluate_split(\n",
    "        eval_model=debiasing_model,  # Base model for embeddings\n",
    "        test_loader=test_loader,\n",
    "        ml_data=ml_data,\n",
    "        cold_users=cold_users,\n",
    "        warm_users=warm_users,\n",
    "        k_values=k_values,\n",
    "        evaluation_func=ndcg_calc_debiased_sampled,\n",
    "        prior_model=dropoutnet,\n",
    "        original_mf=base_model\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDebiased Model Results:\")\n",
    "    print(f\"Debiased NDCGs: {debiased_ndcgs}\")\n",
    "    print(f\"Cold Users (n={debiased_metrics.n_cold_users}):\")\n",
    "    #normalized_cold_ndcgs = normalize_ndcg_by_positives(debiased_metrics.cold_users.ndcg, cold_positives, avg_positives)\n",
    "    print(f\"  Original NDCG@{k_values}: {debiased_metrics.cold_users.ndcg}\")\n",
    "    #print(f\"  Normalized NDCG@{k_values}: {normalized_cold_ndcgs}\")\n",
    "    \n",
    "    print(f\"Warm Users (n={debiased_metrics.n_warm_users}):\")\n",
    "    #normalized_warm_ndcgs = normalize_ndcg_by_positives(debiased_metrics.warm_users.ndcg, warm_positives, avg_positives)\n",
    "    print(f\"  Original NDCG@{k_values}: {debiased_metrics.warm_users.ndcg}\")\n",
    "    #print(f\"  Normalized NDCG@{k_values}: {normalized_warm_ndcgs}\")\n",
    "    \n",
    "    return {\n",
    "        'base': base_metrics,\n",
    "        'dropout': dropout_metrics,\n",
    "        'debiased': debiased_metrics\n",
    "    }\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Dataset loaded with cold_start=True:\n",
      "Train: 692958 interactions\n",
      "Valid: 94109 interactions\n",
      "Test: 213142 interactions\n",
      "Cold-start statistics:\n",
      "Valid items not in train: 370\n",
      "Test items not in train: 741\n",
      "Found 0 cold users and 6037 warm users in test set\n",
      "Cold user ratio: 0.00%\n",
      "\n",
      "Item Split Analysis:\n",
      "Total items in test set: 741\n",
      "Found 741 cold items and 0 warm items\n",
      "Cold item ratio: 100.00%\n",
      "\n",
      "Detailed Statistics:\n",
      "Total unique items in training: 2595\n",
      "Total unique items in test: 741\n",
      "Items in test but not in training: 741\n",
      "Items with insufficient interactions: 0\n",
      "\n",
      "Interaction Statistics:\n",
      "Average interactions per cold item: 287.64\n",
      "Average interactions per warm item: nan\n",
      "\n",
      "Training base model...\n",
      "Epoch 1 - Avg Loss: 1.7384 - Avg Train NDCG: 0.6926 - Avg Test NDCG: 0.0755\n",
      "Epoch 1 - Avg Test Prec: 0.0469 - Avg Test Rec: 0.11290561857830284\n",
      "Base NDCGS: [0.1102781017372421, 0.1328697018896751]\n",
      "Final MDG: 0.22637007809200388, min10: 0.13184730397512892, min20: 0.16022255082586745, top10: 0.31504480025970955\n",
      "\n",
      "Base Model MDG Analysis:\n",
      "\n",
      "MDG Analysis Summary:\n",
      "--------------------------------------------------\n",
      "\n",
      "Coverage Statistics:\n",
      "Total items with MDG scores: 741\n",
      "Cold items with scores: 741\n",
      "Warm items with scores: 0\n",
      "Cold items missing scores: 0\n",
      "Warm items missing scores: 0\n",
      "\n",
      "OVERALL Items:\n",
      "Number of items: 741\n",
      "Mean MDG: 0.2264\n",
      "Median MDG: 0.2267\n",
      "Std Dev: 0.0546\n",
      "\n",
      "COLD Items:\n",
      "Number of items: 741\n",
      "Mean MDG: 0.2264\n",
      "Median MDG: 0.2267\n",
      "Std Dev: 0.0546\n",
      "\n",
      "Evaluating base model...\n",
      "No cold users found, skipping evaluation\n",
      "Evaluating warm users...\n",
      "Evaluating all users...\n",
      "\n",
      "Base Model Results:\n",
      "Base NDCGs: [0.1102781017372421, 0.1328697018896751]\n",
      "\n",
      "Base Model Results:\n",
      "Cold Users (n=0):\n",
      "  Original NDCG@[15, 30]: [0.0, 0.0]\n",
      "Warm Users (n=6037):\n",
      "  Original NDCG@[15, 30]: [0.11094935534469375, 0.13333772805045352]\n",
      "\n",
      "Training DropoutNet model...\n",
      "Initializing DropoutNet training...\n",
      "\tu_concat rank=130\n",
      "\tv_concat rank=118\n",
      "Starting training for 1 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackklawitter/Documents/Rutgers/FairAI/FairColdRec/dropoutnet.py:591: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Uin = torch.tensor(u_emb_expanded[batch_u_idx], device=device)\n",
      "/Users/jackklawitter/Documents/Rutgers/FairAI/FairColdRec/dropoutnet.py:592: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Vin = torch.tensor(i_emb_expanded[batch_i_idx], device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: Average Loss = 0.0494\n",
      "\n",
      "Training completed!\n",
      "\n",
      "Evaluating DropoutNet model...\n",
      "Dropout NDCGs [0.05212612850917899, 0.08856699570013318]\n",
      "Final MDG: 0.08902576936848584, min10: 0.008736681148282802, min20: 0.019503406188051967, top10: 0.19193894259437877\n",
      "\n",
      "Debiased Model MDG Analysis:\n",
      "\n",
      "MDG Analysis Summary:\n",
      "--------------------------------------------------\n",
      "\n",
      "Coverage Statistics:\n",
      "Total items with MDG scores: 741\n",
      "Cold items with scores: 741\n",
      "Warm items with scores: 0\n",
      "Cold items missing scores: 0\n",
      "Warm items missing scores: 0\n",
      "\n",
      "OVERALL Items:\n",
      "Number of items: 741\n",
      "Mean MDG: 0.0890\n",
      "Median MDG: 0.0837\n",
      "Std Dev: 0.0546\n",
      "\n",
      "COLD Items:\n",
      "Number of items: 741\n",
      "Mean MDG: 0.0890\n",
      "Median MDG: 0.0837\n",
      "Std Dev: 0.0546\n",
      "\n",
      "Evaluating DropoutNet model...\n",
      "No cold users found, skipping evaluation\n",
      "Evaluating warm users...\n",
      "Evaluating all users...\n",
      "\n",
      "DropoutNet Results:\n",
      "Dropout NDCGs: [0.05212612850917899, 0.08856699570013318]\n",
      "Cold Users (n=0):\n",
      "  Original NDCG@[15, 30]: [0.0, 0.0]\n",
      "Warm Users (n=6037):\n",
      "  Original NDCG@[15, 30]: [0.052264235403473365, 0.08840031436146094]\n",
      "\n",
      "Training debiasing model...\n",
      "Initializing debiasing model training...\n",
      "Number of users: 6040\n",
      "Number of items: 3706\n",
      "Number of valid warm items: 2595\n",
      "Preprocessing ratings...\n",
      "Number of valid warm items: 2595\n",
      "Output rating matrix shape: torch.Size([3706, 6040])\n",
      "Rating range: [3.5565, 3.6759]\n",
      "Initializing debiasing model...\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Training completed!\n",
      "\n",
      "Evaluating debiased model...\n",
      "Debiased NDCGs [0.04689976903672723, 0.07158455301681681]\n",
      "Final MDG: 0.10014922044150473, min10: 0.0, min20: 0.0, top10: 0.2900659090066803\n",
      "\n",
      "Debiased Model MDG Analysis:\n",
      "\n",
      "MDG Analysis Summary:\n",
      "--------------------------------------------------\n",
      "\n",
      "Coverage Statistics:\n",
      "Total items with MDG scores: 741\n",
      "Cold items with scores: 741\n",
      "Warm items with scores: 0\n",
      "Cold items missing scores: 0\n",
      "Warm items missing scores: 0\n",
      "\n",
      "OVERALL Items:\n",
      "Number of items: 741\n",
      "Mean MDG: 0.1001\n",
      "Median MDG: 0.0994\n",
      "Std Dev: 0.1047\n",
      "\n",
      "COLD Items:\n",
      "Number of items: 741\n",
      "Mean MDG: 0.1001\n",
      "Median MDG: 0.0994\n",
      "Std Dev: 0.1047\n",
      "\n",
      "Evaluating debiased model...\n",
      "No cold users found, skipping evaluation\n",
      "Evaluating warm users...\n",
      "Evaluating all users...\n",
      "\n",
      "Debiased Model Results:\n",
      "Debiased NDCGs: [0.04689976903672723, 0.07158455301681681]\n",
      "Cold Users (n=0):\n",
      "  Original NDCG@[15, 30]: [0.0, 0.0]\n",
      "Warm Users (n=6037):\n",
      "  Original NDCG@[15, 30]: [0.04767893850250609, 0.07171765931429017]\n"
     ]
    }
   ],
   "source": [
    "test = run_evaluation_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

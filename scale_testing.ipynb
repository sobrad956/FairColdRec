{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm(parameters):\n",
    "    \"\"\"Calculate L2 norm of parameters\"\"\"\n",
    "    return torch.sum(torch.square(parameters))\n",
    "\n",
    "\n",
    "class DenseBatchFC(nn.Module):\n",
    "    \"\"\"Dense layer with optional batch normalization\"\"\"\n",
    "    def __init__(self, input_dim, units, do_norm=False):\n",
    "        super(DenseBatchFC, self).__init__()\n",
    "        self.do_norm = do_norm\n",
    "        \n",
    "        # Initialize with same standard deviation as original\n",
    "        self.fc = nn.Linear(input_dim, units)\n",
    "        torch.nn.init.normal_(self.fc.weight, std=0.01)\n",
    "        torch.nn.init.zeros_(self.fc.bias)\n",
    "        \n",
    "        if do_norm:\n",
    "            self.bn = nn.BatchNorm1d(units, momentum=0.1)  # 1-0.9 to match TF's decay=0.9\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        if self.do_norm:\n",
    "            out = self.bn(out)\n",
    "        return out, l2_norm(self.fc.weight) + l2_norm(self.fc.bias)\n",
    "\n",
    "\n",
    "class DebiasingAutoencoder(nn.Module):\n",
    "    def __init__(self, model_select, num_user, num_item, reg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_select: List of hidden layer dimensions\n",
    "            num_user: Number of users\n",
    "            num_item: Number of items\n",
    "            reg: Regularization strength\n",
    "        \"\"\"\n",
    "        super(DebiasingAutoencoder, self).__init__()\n",
    "        self.reg = reg\n",
    "        self.num_user = num_user\n",
    "        self.num_item = num_item\n",
    "        self.model_select = model_select\n",
    "        \n",
    "        # Build encoder layers\n",
    "        layers = []\n",
    "        input_dim = num_user\n",
    "        for hid in model_select:\n",
    "            layers.append(DenseBatchFC(input_dim, hid, do_norm=True))\n",
    "            input_dim = hid\n",
    "        self.encoder_layers = nn.ModuleList(layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(input_dim, num_user)\n",
    "        torch.nn.init.normal_(self.output_layer.weight, std=0.01)\n",
    "        torch.nn.init.zeros_(self.output_layer.bias)\n",
    "        \n",
    "    def forward(self, x, return_reg_loss=True):\n",
    "        last = x\n",
    "        reg_loss = 0\n",
    "        \n",
    "        # Process through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            last, reg = layer(last)\n",
    "            reg_loss += reg\n",
    "            \n",
    "        # Output layer\n",
    "        preds = self.output_layer(last)\n",
    "        if return_reg_loss:\n",
    "            reg_loss += l2_norm(self.output_layer.weight) + l2_norm(self.output_layer.bias)\n",
    "            reg_loss *= self.reg\n",
    "            return preds, reg_loss\n",
    "        return preds\n",
    "    \n",
    "    def get_recommendations(self, R, user_indices, k):\n",
    "        \"\"\"Get top-k recommendations for specified users\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get predictions for all items\n",
    "            preds = self.forward(R, return_reg_loss=False)\n",
    "            \n",
    "            # Select predictions for requested users\n",
    "            user_preds = preds.transpose(0, 1)[user_indices]\n",
    "            \n",
    "            # Get top-k items\n",
    "            _, indices = torch.topk(user_preds, k, dim=1)\n",
    "            return indices\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, learning_rate=0.01, momentum=0.9, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        \n",
    "    def train_step(self, R_input, R_output, optimize_all=True):\n",
    "        \"\"\"\n",
    "        Single training step\n",
    "        Args:\n",
    "            R_input: Input ratings matrix\n",
    "            R_output: Target ratings matrix\n",
    "            optimize_all: If True, optimize both reconstruction and regularization loss\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        R_input = R_input.to(self.device)\n",
    "        R_output = R_output.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        preds, reg_loss = self.model(R_input)\n",
    "        \n",
    "        # Calculate loss\n",
    "        reconstruction_loss = torch.mean(torch.sqrt(torch.sum((preds - R_output) ** 2, dim=1, keepdim=True)))\n",
    "        loss = reconstruction_loss + (reg_loss if optimize_all else 0)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'reconstruction_loss': reconstruction_loss.item(),\n",
    "            'reg_loss': reg_loss.item() if optimize_all else 0,\n",
    "            'total_loss': loss.item()\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, R, eval_data, batch_size=1024):\n",
    "        \"\"\"\n",
    "        Evaluate model on test data\n",
    "        Args:\n",
    "            R: Full ratings matrix\n",
    "            eval_data: Evaluation data containing test_item_ids and test_user_ids\n",
    "            batch_size: Batch size for evaluation\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        R = R.to(self.device)\n",
    "        \n",
    "        recommendations = []\n",
    "        with torch.no_grad():\n",
    "            for start_idx in range(0, len(eval_data.test_user_ids), batch_size):\n",
    "                end_idx = min(start_idx + batch_size, len(eval_data.test_user_ids))\n",
    "                batch_users = torch.tensor(eval_data.test_user_ids[start_idx:end_idx], device=self.device)\n",
    "                \n",
    "                # Get recommendations for batch\n",
    "                batch_recs = self.model.get_recommendations(\n",
    "                    R[eval_data.test_item_ids],\n",
    "                    batch_users,\n",
    "                    k=max(eval_data.recall_at)\n",
    "                )\n",
    "                recommendations.append(batch_recs.cpu())\n",
    "                \n",
    "        return torch.cat(recommendations, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example workings\n",
    "# Initialize model\n",
    "model = DebiasingAutoencoder(\n",
    "    model_select=[256, 128, 64],  # Hidden layer dimensions\n",
    "    num_user=1000,\n",
    "    num_item=500,\n",
    "    reg=0.01\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = ModelTrainer(model)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    losses = trainer.train_step(R_input, R_output)\n",
    "    \n",
    "    # Evaluate periodically\n",
    "    if epoch % eval_interval == 0:\n",
    "        recommendations = trainer.evaluate(R, eval_data)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
